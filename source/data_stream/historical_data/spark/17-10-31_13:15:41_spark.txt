17/10/31 13:15:41 INFO SparkContext: Running Spark version 2.2.0
17/10/31 13:15:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/10/31 13:15:41 INFO SparkContext: Submitted application: W2V
17/10/31 13:15:41 INFO SecurityManager: Changing view acls to: centos
17/10/31 13:15:41 INFO SecurityManager: Changing modify acls to: centos
17/10/31 13:15:41 INFO SecurityManager: Changing view acls groups to: 
17/10/31 13:15:41 INFO SecurityManager: Changing modify acls groups to: 
17/10/31 13:15:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(centos); groups with view permissions: Set(); users  with modify permissions: Set(centos); groups with modify permissions: Set()
17/10/31 13:15:41 INFO Utils: Successfully started service 'sparkDriver' on port 43433.
17/10/31 13:15:41 INFO SparkEnv: Registering MapOutputTracker
17/10/31 13:15:41 INFO SparkEnv: Registering BlockManagerMaster
17/10/31 13:15:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/10/31 13:15:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/10/31 13:15:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-05aa2de6-7f41-493c-a5e5-e1a945ca84b4
17/10/31 13:15:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/10/31 13:15:41 INFO SparkEnv: Registering OutputCommitCoordinator
17/10/31 13:15:42 INFO log: Logging initialized @1499ms
17/10/31 13:15:42 INFO Server: jetty-9.3.z-SNAPSHOT
17/10/31 13:15:42 INFO Server: Started @1578ms
17/10/31 13:15:42 INFO AbstractConnector: Started ServerConnector@7adbd080{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/10/31 13:15:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35038141{/jobs,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7646731d{/jobs/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3b1bb3ab{/jobs/job,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2449cff7{/jobs/job/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62da83ed{/stages,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@37d80fe7{/stages/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e3cee7b{/stages/stage,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@29ad44e3{/stages/stage/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5af9926a{/stages/pool,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@fac80{/stages/pool/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@649f2009{/storage,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69adf72c{/storage/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a15b789{/storage/rdd,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@51650883{/storage/rdd/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5bd1ceca{/environment,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@499b2a5c{/environment/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c1fca1e{/executors,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@344344fa{/executors/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@70e659aa{/executors/threadDump,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@285f09de{/executors/threadDump/json,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31500940{/static,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fdfa676{/,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5be82d43{/api,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1734f68{/jobs/job/kill,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5ed190be{/stages/stage/kill,null,AVAILABLE,@Spark}
17/10/31 13:15:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.19:4040
17/10/31 13:15:42 INFO SparkContext: Added JAR file:/home/centos/sparksample/target/scala-2.10/spark-sample_2.10-1.0.jar at spark://10.0.0.19:43433/jars/spark-sample_2.10-1.0.jar with timestamp 1509430542186
17/10/31 13:15:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://ssw1-sea-master-0.novalocal:7077...
17/10/31 13:15:42 INFO TransportClientFactory: Successfully created connection to ssw1-sea-master-0.novalocal/10.0.0.19:7077 after 23 ms (0 ms spent in bootstraps)
17/10/31 13:15:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20171031131542-0028
17/10/31 13:15:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171031131542-0028/0 on worker-20171030154439-10.0.0.5-33804 (10.0.0.5:33804) with 4 cores
17/10/31 13:15:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20171031131542-0028/0 on hostPort 10.0.0.5:33804 with 4 cores, 1024.0 MB RAM
17/10/31 13:15:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171031131542-0028/1 on worker-20171030154435-10.0.0.21-37478 (10.0.0.21:37478) with 4 cores
17/10/31 13:15:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38866.
17/10/31 13:15:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20171031131542-0028/1 on hostPort 10.0.0.21:37478 with 4 cores, 1024.0 MB RAM
17/10/31 13:15:42 INFO NettyBlockTransferService: Server created on 10.0.0.19:38866
17/10/31 13:15:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/10/31 13:15:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.19, 38866, None)
17/10/31 13:15:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.19:38866 with 366.3 MB RAM, BlockManagerId(driver, 10.0.0.19, 38866, None)
17/10/31 13:15:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.19, 38866, None)
17/10/31 13:15:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.19, 38866, None)
17/10/31 13:15:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171031131542-0028/1 is now RUNNING
17/10/31 13:15:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171031131542-0028/0 is now RUNNING
17/10/31 13:15:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4d6f197e{/metrics/json,null,AVAILABLE,@Spark}
17/10/31 13:15:43 INFO EventLoggingListener: Logging events to hdfs:///tmp/spark-events/app-20171031131542-0028
17/10/31 13:15:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/10/31 13:15:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 240.1 KB, free 366.1 MB)
17/10/31 13:15:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.21:32824) with ID 1
17/10/31 13:15:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.5 KB, free 366.0 MB)
17/10/31 13:15:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.19:38866 (size: 23.5 KB, free: 366.3 MB)
17/10/31 13:15:43 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.21:44849 with 366.3 MB RAM, BlockManagerId(1, 10.0.0.21, 44849, None)
17/10/31 13:15:43 INFO SparkContext: Created broadcast 0 from textFile at W2V1.scala:7
17/10/31 13:15:44 INFO FileInputFormat: Total input paths to process : 1
17/10/31 13:15:44 INFO SparkContext: Starting job: collect at Word2Vec.scala:196
17/10/31 13:15:44 INFO DAGScheduler: Registering RDD 4 (map at Word2Vec.scala:187)
17/10/31 13:15:44 INFO DAGScheduler: Got job 0 (collect at Word2Vec.scala:196) with 4 output partitions
17/10/31 13:15:44 INFO DAGScheduler: Final stage: ResultStage 1 (collect at Word2Vec.scala:196)
17/10/31 13:15:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/10/31 13:15:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/10/31 13:15:44 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at map at Word2Vec.scala:187), which has no missing parents
17/10/31 13:15:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.5:51128) with ID 0
